# SupaQuery - Complete Project Documentation

## üéØ Project Overview

**SupaQuery** is an intelligent document analysis and question-answering system that combines **Graph-based Retrieval Augmented Generation (GraphRAG)** with **Knowledge Graph** technology. It allows users to upload documents (PDF, DOCX, images, audio) and ask natural language questions about their content, receiving contextually accurate answers powered by AI.

### Key Differentiators
- **Knowledge Graph Architecture**: Uses Memgraph to build relationships between document entities
- **Entity-Aware Retrieval**: Extracts and indexes named entities (people, organizations, locations, etc.)
- **Role-Based Access Control (RBAC)**: Enterprise-grade permission system
- **Local-First AI**: Runs entirely on your infrastructure using Ollama (no external API calls)
- **Multi-Modal Support**: Processes text, images (OCR), and audio (transcription)

---

## üèóÔ∏è System Architecture

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        FRONTEND (Next.js)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ   Upload UI  ‚îÇ  ‚îÇ   Chat UI    ‚îÇ  ‚îÇ  Auth/RBAC   ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ JWT Auth
                              ‚îÇ HTTP/REST API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      BACKEND (FastAPI)                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ              Document Processor Service                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ PDF Parser   ‚Ä¢ DOCX Parser   ‚Ä¢ Image OCR              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Audio Transcription   ‚Ä¢ Text Chunking                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                 GraphRAG Service                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Entity Extraction (spaCy)                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Query Processing   ‚Ä¢ Context Building                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ LLM Integration    ‚Ä¢ Answer Generation                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                      ‚îÇ                      ‚îÇ
           ‚îÇ                      ‚îÇ                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL       ‚îÇ  ‚îÇ    Memgraph      ‚îÇ  ‚îÇ     Ollama       ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   Users     ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Documents ‚îÇ   ‚îÇ  ‚îÇ  ‚îÇ  llama3.2 ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Roles     ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Chunks   ‚îÇ   ‚îÇ  ‚îÇ  ‚îÇ    LLM    ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ Permissions ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Entities  ‚îÇ   ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ   ‚îÇ Documents   ‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  ‚îÇ                  ‚îÇ
‚îÇ   ‚îÇ  Metadata   ‚îÇ  ‚îÇ  ‚îÇ  Knowledge Graph ‚îÇ  ‚îÇ  Local Inference ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  Auth & Metadata   ‚îÇ   Semantic Network    ‚îÇ   AI Processing   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   (120x faster!)       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Technology Stack

### Frontend
- **Framework**: Next.js 14 with App Router
- **Language**: TypeScript
- **UI Components**: shadcn/ui (Radix UI primitives)
- **Styling**: Tailwind CSS
- **State Management**: React Hooks + Context API
- **Authentication**: JWT with localStorage

### Backend
- **Framework**: FastAPI (Python 3.13)
- **API Style**: RESTful with async/await
- **Authentication**: JWT (JSON Web Tokens)
- **Document Processing**:
  - PyPDF2 (PDF parsing)
  - python-docx (DOCX parsing)
  - pytesseract (OCR for images)
  - whisper (audio transcription)

### Databases
- **PostgreSQL**: User authentication, RBAC, document metadata
  - SQLAlchemy ORM with async support
  - Connection pooling
  - ACID transactions
  
- **Memgraph**: Knowledge graph for document relationships
  - Graph database (120x faster than Neo4j)
  - GQLAlchemy Python client
  - Cypher query language
  - Real-time graph traversal

### AI/ML Components
- **Ollama**: Local LLM inference (llama3.2)
- **spaCy**: Named Entity Recognition (NER)
  - Model: en_core_web_sm
  - 18 entity types supported
- **Simple Hash Embeddings**: No external ML dependencies

---

## üì¶ System Components

### 1. Authentication & Authorization System

#### JWT Authentication
```python
# Token Structure
{
  "sub": "user_id",           # Subject (user identifier)
  "username": "john",         # Username
  "exp": 1234567890,          # Expiration timestamp
  "iat": 1234567000           # Issued at timestamp
}
```

#### RBAC (Role-Based Access Control)
```
Roles Hierarchy:
‚îú‚îÄ‚îÄ admin (superuser)
‚îÇ   ‚îú‚îÄ‚îÄ All permissions
‚îÇ   ‚îú‚îÄ‚îÄ User management
‚îÇ   ‚îî‚îÄ‚îÄ Role management
‚îÇ
‚îú‚îÄ‚îÄ user (standard)
‚îÇ   ‚îú‚îÄ‚îÄ documents:create/read/update/delete
‚îÇ   ‚îú‚îÄ‚îÄ chat:create/read/delete
‚îÇ   ‚îî‚îÄ‚îÄ Limited to own resources
‚îÇ
‚îî‚îÄ‚îÄ viewer (read-only)
    ‚îú‚îÄ‚îÄ documents:read
    ‚îî‚îÄ‚îÄ chat:read
```

**Permission System**:
- **Resources**: documents, chat, users, roles
- **Actions**: create, read, update, delete, share, manage
- **Enforcement**: FastAPI dependencies on every endpoint
- **Ownership**: Documents scoped to user_id + shared access

---

### 2. Document Processing Pipeline

#### Upload Flow
```
1. File Upload (frontend)
   ‚Üì
2. Save to Disk (/uploads/{uuid}.ext)
   ‚Üì
3. Content Extraction
   ‚îÇ
   ‚îú‚îÄ‚Üí PDF: PyPDF2 ‚Üí text extraction
   ‚îú‚îÄ‚Üí DOCX: python-docx ‚Üí text extraction
   ‚îú‚îÄ‚Üí Image: pytesseract ‚Üí OCR
   ‚îî‚îÄ‚Üí Audio: whisper ‚Üí transcription
   ‚Üì
4. Text Chunking
   - Chunk size: 512 characters
   - Overlap: 50 characters
   - Preserve sentence boundaries
   ‚Üì
5. Entity Extraction (spaCy)
   - Named entities: PERSON, ORG, GPE, etc.
   - Concepts: Noun chunks
   - Context preservation
   ‚Üì
6. Database Storage
   ‚îú‚îÄ‚Üí PostgreSQL: Metadata (filename, size, user_id, status)
   ‚îî‚îÄ‚Üí Memgraph: Knowledge graph (chunks + entities)
   ‚Üì
7. Success Response
```

#### Supported File Types
| Type | Extensions | Processing Method |
|------|-----------|-------------------|
| PDF | .pdf | PyPDF2 text extraction |
| DOCX | .docx | python-docx DOM parsing |
| Images | .jpg, .png, .jpeg, .gif | Tesseract OCR |
| Audio | .mp3, .wav, .ogg, .m4a | Whisper transcription |

---

### 3. Knowledge Graph Architecture (Memgraph)

#### Graph Schema

```cypher
# Node Types
(:Document {
  id: string,              # Unique document identifier
  filename: string,        # Original filename
  user_id: integer,        # Owner user ID
  created_at: datetime,    # Upload timestamp
  file_type: string        # pdf/docx/image/audio
})

(:Chunk {
  id: string,              # Unique chunk identifier
  text: string,            # Chunk content
  chunk_index: integer,    # Position in document
  embedding_hash: string,  # Hash of content (for similarity)
  created_at: datetime
})

(:Entity {
  name: string,            # Entity text (e.g., "Apple Inc.")
  type: string,            # Entity type (ORG, PERSON, GPE, etc.)
  mention_count: integer,  # Frequency across corpus
  first_seen: datetime
})

# Relationship Types
(Document)-[:CONTAINS]->(Chunk)
(Chunk)-[:MENTIONS {context: string}]->(Entity)
(Entity)-[:RELATES_TO {strength: float}]->(Entity)
```

#### Entity Types (18 Total)
- **PERSON**: People, including fictional characters
- **ORG**: Organizations, companies, agencies
- **GPE**: Geopolitical entities (countries, cities, states)
- **LOC**: Non-GPE locations (mountain ranges, bodies of water)
- **PRODUCT**: Products, vehicles, foods
- **EVENT**: Named events (wars, conferences, sports events)
- **WORK_OF_ART**: Titles of books, songs, movies
- **LAW**: Named documents made into laws
- **LANGUAGE**: Named languages
- **DATE**: Absolute or relative dates
- **TIME**: Times smaller than a day
- **PERCENT**: Percentage values
- **MONEY**: Monetary values
- **QUANTITY**: Measurements
- **ORDINAL**: First, second, third, etc.
- **CARDINAL**: Numerals not covered by other types
- **NORP**: Nationalities, religious/political groups
- **FAC**: Buildings, airports, highways, bridges

#### Graph Indexes
```cypher
CREATE INDEX ON :Document(id);
CREATE INDEX ON :Chunk(id);
CREATE INDEX ON :Entity(name);
CREATE INDEX ON :Concept(name);
CREATE INDEX ON :User(id);
```

---

### 4. GraphRAG Query Pipeline

#### Query Processing Flow

```
1. User Query Input
   "What did Apple announce in the document?"
   ‚Üì
2. Entity Extraction
   - Extract: ["Apple"]
   - Type: ORG
   ‚Üì
3. Entity Search (Memgraph)
   MATCH (e:Entity {name: "Apple"})
   RETURN e
   ‚Üì
4. Find Related Chunks
   MATCH (e:Entity {name: "Apple"})<-[:MENTIONS]-(c:Chunk)
   RETURN c.text, c.chunk_index
   ORDER BY mention_count DESC
   LIMIT 10
   ‚Üì
5. Build Context
   context = """
   Relevant information from documents:
   
   Document: report.pdf, Chunk 3:
   "Apple Inc. announced new iPhone models..."
   
   Document: report.pdf, Chunk 7:
   "The company's revenue exceeded expectations..."
   """
   ‚Üì
6. LLM Prompt Construction
   system_prompt + context + user_query
   ‚Üì
7. Ollama Inference (llama3.2)
   - Temperature: 0.3 (focused responses)
   - Context window: 2048 tokens
   - Max tokens: 512
   ‚Üì
8. Response Generation
   {
     "answer": "Apple announced...",
     "citations": ["report.pdf"],
     "entities": ["Apple"],
     "sources": [{"doc_id": "...", "chunk": 3}]
   }
```

#### System Prompt

```text
You are SupaQuery, an AI assistant specialized in analyzing documents 
using a knowledge graph.

Core Principles:
1. ACCURACY FIRST: Only answer based on provided document context
2. CITE SOURCES: Reference specific documents and sections
3. ENTITY AWARENESS: Use extracted entities to provide precise answers
4. ADMIT UNCERTAINTY: Say "I don't have information about that" 
   when context is insufficient
5. STRUCTURED RESPONSES: Organize answers clearly with bullet points

Available Context:
- Document chunks with extracted entities
- Relationships between entities
- Document metadata (filename, upload date, owner)

Response Format:
- Direct answer first
- Supporting evidence with citations
- Related entities mentioned
- Confidence level based on context availability
```

---

## üîÑ Complete User Journey

### Scenario: Analyzing Company Reports

#### Step 1: User Registration & Login
```
1. User navigates to /signup
2. Enters: username, email, password, full_name
3. Backend creates user with 'user' role
4. Auto-login after registration
5. JWT token stored in localStorage
6. Redirect to main dashboard
```

#### Step 2: Document Upload
```
Frontend:
1. User drags PDF file to upload zone
2. JavaScript reads file as FormData
3. Retrieves JWT token from localStorage
4. POST /api/upload with Authorization header

Backend:
1. Verify JWT token & 'documents:create' permission
2. Save file: /uploads/{uuid}.pdf
3. Extract text with PyPDF2
4. Chunk into 512-char segments with 50-char overlap
5. Extract entities with spaCy (e.g., "Microsoft", "Q3 2024")
6. Save metadata to PostgreSQL
7. Add to Memgraph graph:
   - Create Document node
   - Create Chunk nodes
   - Create Entity nodes
   - Link: Document‚ÜíCONTAINS‚ÜíChunk
   - Link: Chunk‚ÜíMENTIONS‚ÜíEntity
8. Return success response
```

#### Step 3: Query Documents
```
User Query: "What was Microsoft's revenue in Q3?"

Frontend:
1. User types query in chat input
2. POST /api/chat with JWT token

Backend:
1. Verify JWT & 'chat:create' permission
2. Extract entities from query: ["Microsoft", "Q3"]
3. Query Memgraph:
   MATCH (e:Entity)<-[:MENTIONS]-(c:Chunk)
   WHERE e.name IN ["Microsoft", "Q3"]
   RETURN c.text, c.chunk_index
   LIMIT 10
4. Build context from retrieved chunks
5. Send to Ollama LLM:
   - System prompt
   - Document context
   - User query
6. Generate answer
7. Return response with citations

Frontend:
1. Display answer in chat UI
2. Show citations (clickable document links)
3. Highlight mentioned entities
```

#### Step 4: Explore Knowledge Graph
```
1. User opens Memgraph Lab UI: http://localhost:3001
2. Run Cypher query:
   MATCH (d:Document)-[:CONTAINS]->(c:Chunk)-[:MENTIONS]->(e:Entity)
   WHERE d.id = "document_uuid"
   RETURN d, c, e
   LIMIT 50
3. Visual graph display:
   - Document node (center)
   - Chunk nodes (surrounding)
   - Entity nodes (outer ring)
   - Relationships shown as arrows
4. Click entity to see all documents mentioning it
5. Analyze entity co-occurrence patterns
```

---

## üóÑÔ∏è Database Schemas

### PostgreSQL Schema

```sql
-- Users table
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    full_name VARCHAR(100),
    is_active BOOLEAN DEFAULT TRUE,
    is_superuser BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Roles table
CREATE TABLE roles (
    id SERIAL PRIMARY KEY,
    name VARCHAR(50) UNIQUE NOT NULL,
    description TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Permissions table
CREATE TABLE permissions (
    id SERIAL PRIMARY KEY,
    resource VARCHAR(50) NOT NULL,      -- 'documents', 'chat', etc.
    action VARCHAR(50) NOT NULL,        -- 'create', 'read', etc.
    description TEXT,
    UNIQUE(resource, action)
);

-- User-Role mapping (many-to-many)
CREATE TABLE user_roles (
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
    role_id INTEGER REFERENCES roles(id) ON DELETE CASCADE,
    assigned_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (user_id, role_id)
);

-- Role-Permission mapping (many-to-many)
CREATE TABLE role_permissions (
    role_id INTEGER REFERENCES roles(id) ON DELETE CASCADE,
    permission_id INTEGER REFERENCES permissions(id) ON DELETE CASCADE,
    PRIMARY KEY (role_id, permission_id)
);

-- Documents table
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
    filename VARCHAR(255) NOT NULL,
    original_filename VARCHAR(255) NOT NULL,
    file_type VARCHAR(50),
    file_size BIGINT,
    file_path TEXT,
    status VARCHAR(50) DEFAULT 'processing',
    total_chunks INTEGER DEFAULT 0,
    is_public BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Document chunks (for reference)
CREATE TABLE document_chunks (
    id SERIAL PRIMARY KEY,
    document_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    text TEXT NOT NULL,
    embedding_hash VARCHAR(64),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Chat sessions
CREATE TABLE chat_sessions (
    id UUID PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
    title VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Chat messages
CREATE TABLE chat_messages (
    id SERIAL PRIMARY KEY,
    session_id UUID REFERENCES chat_sessions(id) ON DELETE CASCADE,
    role VARCHAR(20) NOT NULL,          -- 'user' or 'assistant'
    content TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_documents_user_id ON documents(user_id);
CREATE INDEX idx_documents_created_at ON documents(created_at);
CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chat_sessions_user_id ON chat_sessions(user_id);
CREATE INDEX idx_chat_messages_session_id ON chat_messages(session_id);
```

---

## üîå API Endpoints

### Authentication Endpoints

#### POST `/api/auth/register`
Register a new user
```json
Request:
{
  "username": "john_doe",
  "email": "john@example.com",
  "password": "SecurePass123!",
  "full_name": "John Doe"
}

Response:
{
  "id": 1,
  "username": "john_doe",
  "email": "john@example.com",
  "full_name": "John Doe",
  "is_active": true,
  "roles": ["user"]
}
```

#### POST `/api/auth/login`
Authenticate and get JWT token
```json
Request:
{
  "username": "john_doe",
  "password": "SecurePass123!"
}

Response:
{
  "access_token": "eyJhbGciOiJIUzI1NiIs...",
  "token_type": "bearer",
  "user": {
    "id": 1,
    "username": "john_doe",
    "roles": ["user"]
  }
}
```

### Document Endpoints

#### POST `/api/upload`
Upload and process documents
```
Headers:
  Authorization: Bearer {token}
  Content-Type: multipart/form-data

Body:
  files: [File, File, ...]
  is_public: boolean (optional)

Response:
{
  "success": true,
  "files": [
    {
      "id": 1,
      "file_id": "uuid",
      "name": "report.pdf",
      "type": "pdf",
      "size": 1024000,
      "status": "processed",
      "chunks": 45,
      "is_public": false
    }
  ]
}
```

#### GET `/api/documents`
List user's documents
```
Headers:
  Authorization: Bearer {token}

Query Params:
  skip: integer (default: 0)
  limit: integer (default: 100)

Response:
{
  "documents": [
    {
      "id": 1,
      "filename": "report.pdf",
      "original_filename": "Q3_Report.pdf",
      "file_type": "pdf",
      "file_size": 1024000,
      "status": "processed",
      "total_chunks": 45,
      "created_at": "2025-10-04T10:30:00Z"
    }
  ],
  "total": 1
}
```

#### DELETE `/api/documents/{id}`
Delete a document
```
Headers:
  Authorization: Bearer {token}

Response:
{
  "success": true,
  "message": "Document deleted successfully"
}
```

### Chat Endpoints

#### POST `/api/chat`
Query documents with natural language
```json
Headers:
  Authorization: Bearer {token}
  Content-Type: application/json

Request:
{
  "query": "What is the main topic of the document?",
  "session_id": null,
  "document_ids": [1, 2]  // optional: specific documents
}

Response:
{
  "answer": "The main topic is...",
  "citations": ["report.pdf", "notes.docx"],
  "entities": [
    {"name": "Apple Inc.", "type": "ORG"},
    {"name": "Q3 2024", "type": "DATE"}
  ],
  "sources": [
    {
      "document_id": 1,
      "chunk_index": 3,
      "text": "Relevant excerpt..."
    }
  ],
  "session_id": "uuid"
}
```

### User Endpoints

#### GET `/api/users/me`
Get current user info
```
Headers:
  Authorization: Bearer {token}

Response:
{
  "id": 1,
  "username": "john_doe",
  "email": "john@example.com",
  "full_name": "John Doe",
  "is_active": true,
  "is_superuser": false,
  "roles": ["user"],
  "created_at": "2025-10-01T12:00:00Z"
}
```

#### GET `/api/users/me/permissions`
Get user's permissions (debug endpoint)
```
Headers:
  Authorization: Bearer {token}

Response:
{
  "username": "john_doe",
  "roles": ["user"],
  "permissions": [
    "documents:create",
    "documents:read",
    "documents:update",
    "documents:delete",
    "chat:create",
    "chat:read"
  ],
  "is_superuser": false
}
```

---

## ‚öôÔ∏è Configuration & Environment

### Backend Environment Variables (`.env`)

```bash
# PostgreSQL Configuration
DATABASE_URL=postgresql+asyncpg://mac@localhost/supaquery

# JWT Authentication
SECRET_KEY=your-secret-key-here
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest

# Memgraph Configuration
MEMGRAPH_HOST=localhost
MEMGRAPH_PORT=7687

# Server Configuration
BACKEND_PORT=8000
BACKEND_HOST=0.0.0.0

# CORS Origins
CORS_ORIGINS=http://localhost:3000,http://frontend:3000

# Storage Paths
UPLOAD_DIR=./uploads
STORAGE_DIR=./storage

# Processing Configuration
CHUNK_SIZE=512
CHUNK_OVERLAP=50
MAX_FILE_SIZE=52428800  # 50MB

# Tesseract (for OCR)
TESSERACT_CMD=/opt/homebrew/bin/tesseract  # macOS
# TESSERACT_CMD=/usr/bin/tesseract          # Linux

# Whisper Model
WHISPER_MODEL=base  # options: tiny, base, small, medium, large
```

### Frontend Environment Variables

```bash
NEXT_PUBLIC_API_URL=http://localhost:8000
```

---

## üöÄ Deployment & Scaling

### Development Setup

```bash
# 1. Start PostgreSQL
brew services start postgresql  # macOS
# OR
sudo systemctl start postgresql  # Linux

# 2. Start Memgraph
docker run -d \
  --name memgraph \
  -p 7687:7687 \
  -p 7444:7444 \
  -p 3001:3000 \
  -v mg_data:/var/lib/memgraph \
  -v mg_log:/var/log/memgraph \
  -v mg_etc:/etc/memgraph \
  memgraph/memgraph-platform:latest

# 3. Start Ollama
ollama serve  # Terminal 1
ollama pull llama3.2  # Terminal 2

# 4. Initialize Backend
cd backend
python -m venv venv
source venv/bin/activate  # Linux/macOS
# OR venv\Scripts\activate  # Windows
pip install -r requirements.txt
python -m spacy download en_core_web_sm
python init_db.py  # Create tables, roles, admin user
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# 5. Start Frontend
cd frontend
npm install
npm run dev  # http://localhost:3000
```

### Production Considerations

#### 1. Database Connection Pooling
```python
# PostgreSQL
engine = create_async_engine(
    DATABASE_URL,
    pool_size=20,        # Connection pool size
    max_overflow=40,     # Max overflow connections
    pool_timeout=30,     # Wait timeout
    pool_recycle=3600    # Recycle connections after 1 hour
)
```

#### 2. API Rate Limiting
```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/chat")
@limiter.limit("10/minute")  # 10 requests per minute
async def chat(request: Request, ...):
    pass
```

#### 3. Caching Strategy
```python
# Redis for session caching
import redis.asyncio as redis

redis_client = redis.from_url("redis://localhost:6379")

# Cache user permissions
@lru_cache(maxsize=1000)
def get_user_permissions(user_id: int):
    # Cache for 5 minutes
    pass
```

#### 4. Load Balancing
```nginx
# Nginx configuration
upstream backend {
    least_conn;
    server backend1:8000;
    server backend2:8000;
    server backend3:8000;
}

server {
    listen 80;
    location / {
        proxy_pass http://backend;
    }
}
```

#### 5. Monitoring & Logging
```python
import logging
from prometheus_client import Counter, Histogram

# Metrics
request_counter = Counter('api_requests_total', 'Total API requests')
response_time = Histogram('api_response_time_seconds', 'Response time')

# Structured logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
```

---

## üîí Security Features

### 1. Authentication Security
- **Password Hashing**: bcrypt with salt
- **JWT Tokens**: Signed with HS256
- **Token Expiration**: 30 minutes default
- **Secure Headers**: CORS, CSP configured

### 2. Authorization Security
- **RBAC Enforcement**: Every endpoint protected
- **Ownership Checks**: Users can only access their resources
- **Permission Validation**: Database-backed permission checks

### 3. Input Validation
- **File Type Validation**: Whitelist of allowed extensions
- **File Size Limits**: 50MB default maximum
- **SQL Injection Prevention**: Parameterized queries with SQLAlchemy
- **XSS Prevention**: Content sanitization in responses

### 4. Data Privacy
- **Document Isolation**: Users can only see their own documents
- **Shared Documents**: Explicit sharing mechanism
- **Public Documents**: Opt-in visibility flag

---

## üìä Performance Characteristics

### Response Times (Typical)

| Operation | Time | Notes |
|-----------|------|-------|
| User Login | 50-100ms | JWT generation |
| Document Upload (1MB) | 2-5s | Including processing |
| Entity Extraction | 100-500ms | Per 1000 words |
| Graph Query | 50-200ms | Memgraph traversal |
| LLM Inference | 2-10s | Depends on context size |
| Full Chat Response | 3-12s | End-to-end |

### Scalability Limits

| Resource | Limit | Scaling Strategy |
|----------|-------|------------------|
| Concurrent Users | 100-500 | Add backend instances |
| Documents per User | 10,000+ | Graph partitioning |
| Total Documents | 1M+ | Shard Memgraph |
| Queries per Second | 50-200 | Cache + load balancing |
| Document Size | 50MB | Chunked processing |

### Memory Usage

```
Backend Process:
- Base: ~200MB (FastAPI + libraries)
- Per Request: +5-20MB (document processing)
- spaCy Model: ~50MB (loaded once)
- Connection Pools: ~50MB

Memgraph:
- Base: ~100MB
- Per 1000 Documents: ~50-100MB
- Per 10K Entities: ~10-20MB
- Indexes: ~10% of data size

Ollama:
- llama3.2 Model: ~2GB (loaded once)
- Per Inference: +50-200MB (context window)
```

---

## üêõ Troubleshooting

### Common Issues

#### 1. Database Connection Failed
```
Error: role "postgres" does not exist

Solution:
1. Check DATABASE_URL in .env
2. Ensure PostgreSQL is running:
   brew services status postgresql  # macOS
   sudo systemctl status postgresql  # Linux
3. Create database:
   createdb supaquery
```

#### 2. Memgraph Connection Error
```
Error: Connection refused to localhost:7687

Solution:
1. Check Docker container:
   docker ps | grep memgraph
2. Restart if needed:
   docker start memgraph
3. Check logs:
   docker logs memgraph
```

#### 3. Ollama Not Responding
```
Error: Connection refused to localhost:11434

Solution:
1. Start Ollama:
   ollama serve
2. Pull model:
   ollama pull llama3.2
3. Test:
   ollama run llama3.2 "Hello"
```

#### 4. 403 Forbidden on Upload
```
Error: Permission denied: documents:create

Solution:
1. Check JWT token in localStorage
2. Verify user has 'user' or 'admin' role
3. Run:
   python init_db.py  # Reset permissions
```

#### 5. spaCy Model Not Found
```
Error: Can't find model 'en_core_web_sm'

Solution:
python -m spacy download en_core_web_sm
```

---

## üîÆ Future Enhancements

### Planned Features

1. **Advanced Search**
   - Faceted search by entity type
   - Date range filtering
   - Full-text search with relevance ranking

2. **Collaborative Features**
   - Team workspaces
   - Document annotations
   - Shared chat sessions

3. **Enhanced Entity Extraction**
   - Custom entity types
   - Entity disambiguation
   - Relationship extraction

4. **Multi-Language Support**
   - Language detection
   - Translation integration
   - Multi-lingual entity recognition

5. **Advanced Visualizations**
   - Interactive graph explorer
   - Entity timeline view
   - Document similarity clustering

6. **Export & Integration**
   - Export to JSON/CSV
   - Webhook notifications
   - REST API for third-party integration

7. **Performance Optimization**
   - Vector embeddings (FAISS optional)
   - Query result caching
   - Incremental indexing

---

## üìù Development Guidelines

### Code Style
- **Backend**: PEP 8 (Python), Black formatter
- **Frontend**: ESLint + Prettier, TypeScript strict mode
- **Commits**: Conventional Commits format

### Testing Strategy
```bash
# Backend Tests
pytest tests/ -v --cov=app

# Frontend Tests
npm run test
npm run test:e2e

# Integration Tests
pytest tests/integration/ -v
```

### Documentation
- API docs: Auto-generated by FastAPI (http://localhost:8000/docs)
- Code comments: Docstrings for all public functions
- Architecture diagrams: Mermaid in markdown

---

## üìû Support & Resources

### Documentation
- **FastAPI Docs**: https://fastapi.tiangolo.com
- **Next.js Docs**: https://nextjs.org/docs
- **Memgraph Docs**: https://memgraph.com/docs
- **Ollama Docs**: https://ollama.ai/docs

### Community
- GitHub Issues: Report bugs and feature requests
- Discussions: Architecture questions and ideas

### License
This project is proprietary software. All rights reserved.

---

## üéì Learning Resources

### For Understanding the Project

1. **GraphRAG Concept**
   - Microsoft GraphRAG paper
   - Knowledge graphs in NLP
   - RAG vs Fine-tuning comparison

2. **Graph Databases**
   - Memgraph vs Neo4j comparison
   - Cypher query language tutorial
   - Graph data modeling patterns

3. **FastAPI & Modern Python**
   - Async/await in Python
   - Dependency injection pattern
   - SQLAlchemy async ORM

4. **Next.js & React**
   - Server components vs Client components
   - App Router architecture
   - TypeScript best practices

5. **Authentication & Security**
   - JWT authentication flow
   - RBAC implementation patterns
   - OAuth 2.0 basics

---

## üìà Project Statistics

```
Total Lines of Code: ~15,000
‚îú‚îÄ‚îÄ Backend: ~8,000 (Python)
‚îú‚îÄ‚îÄ Frontend: ~6,000 (TypeScript/React)
‚îî‚îÄ‚îÄ Config/Docs: ~1,000 (YAML/Markdown)

Files: ~150
‚îú‚îÄ‚îÄ Backend Components: 40
‚îú‚îÄ‚îÄ Frontend Components: 60
‚îú‚îÄ‚îÄ Tests: 30
‚îî‚îÄ‚îÄ Documentation: 20

Dependencies:
‚îú‚îÄ‚îÄ Backend: 35 packages
‚îî‚îÄ‚îÄ Frontend: 45 packages

Database Tables: 10
Graph Node Types: 3
Graph Relationship Types: 3
API Endpoints: 25
```

---

## üèÅ Quick Start Summary

```bash
# Clone and setup
git clone <repository>
cd SupaQuery

# Backend setup
cd backend
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
python -m spacy download en_core_web_sm
cp .env.example .env  # Edit with your settings
python init_db.py

# Start services
docker run -d --name memgraph -p 7687:7687 -p 3001:3000 memgraph/memgraph-platform
ollama serve &
ollama pull llama3.2

# Run backend
uvicorn main:app --reload

# Frontend setup (new terminal)
cd frontend
npm install
npm run dev

# Access
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
# Memgraph Lab: http://localhost:3001
```

---

**SupaQuery** - Intelligent Document Analysis with Knowledge Graphs üöÄ

*Last Updated: October 4, 2025*
